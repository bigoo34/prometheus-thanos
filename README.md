# DEPLOYMENT 
Deploying Prometheus Services Accounts, Clusterrole and Clusterrolebinding: The following manifest creates the monitoring namespace, service accounts, clusterrole and clusterrolebindings needed by Prometheus. 
~~~
kubectl apply -f 01-deploy.yaml
~~~

# Deploying Prometheus Configuration configmap: 
The following config map creates the Prometheus configuration file template that will be read by the Thanos sidecar component. The template will also generate the actual configuration file. The file will be consumed by the Prometheus container running in the same pod. It is extremely important to add the external_labels section in the config file so that the querier can deduplicate data based on it.

~~~
kubectl apply -f 01-configmap.yaml
~~~

# Deploying Prometheus Rules configmap
this will create alert rules that will be relayed to Alertmanager for delivery.

~~~
kubectl apply -f 03-configmap-alerts.yaml
~~~

# Deploying Prometheus Stateful Set

~~~
kubectl apply -f 04-statefulset.yaml
~~~

It is important to understand the following about the above manifest:

Prometheus is deployed as a stateful set with three replicas. Each replica provisions its own persistent volume dynamically. 
Prometheus configuration is generated by the Thanos Sidecar container using the template file created above.
Thanos handles data compaction and therefore we need to set --storage.tsdb.min-block-duration=2h and --storage.tsdb.max-block-duration=2h  
Prometheus stateful set is labeled as thanos-store-api: "true" so that each pod gets discovered by the headless service (we will show you how to do that next). This headless service will be used by Thanos Query to query data across all the Prometheus instances. 
We apply the same label to the Thanos Store and Thanos Ruler component so that they are also discovered by the querier and can be used for querying metrics. 
The GCS bucket credentials path is provided using the GOOGLE_APPLICATION_CREDENTIALS environment variable. The configuration file is mounted to that from the secret created as a part of the prerequisites. 


# Deploying Prometheus Services
~~~
kubectl apply -f 05-svc.yaml
~~~

We create different services for each Prometheus pod in the stateful set. These are not strictly necessary, but are created only for debugging purposes. The purpose of thanos-store-gateway headless service has been explained above. Next, we will expose the Prometheus services using an ingress object. 


# Deploying Thanos Query: 
this is one of the main components of Thanos deployment. Note the following

The container argument --store=dnssrv+thanos-store-gateway:10901 helps discover all the components from which metric data should be queried.
The service thanos-querier provides a web interface to run PromQL queries. It also has the option to deduplicate data across various Prometheus clusters. 
From here, we provide Grafana as a datasource for all the dashboards.

~~~
kubectl apply -f 06-thanos-query.yaml
~~~

# Deploying Thanos Ruler
~~~
kubectl apply -f 07-thanos-ruler.yaml
~~~

# Deploying Alertmanager: 
This will create our alertmanager deployment. It will deliver all the alerts generated as per Prometheus Rules.

~~~
kubectl apply -f 08-alertmanager.yaml
~~~

# Deploying Kubestate Metrics:
Kubestate metrics deployment is needed to relay some important container metrics. These metrics are not natively exposed by the kubelet and are not directly available to Prometheus.

~~~
kubectl apply -f 09-kubestate-metrics.yaml
~~~

# Deploying Node-exporter Daemonset:
Node-exporter daemonset runs a node-exporter pod on each node. It exposes very important node metrics that can be pulled by Prometheus instances. 

~~~
kubectl apply -f 10-node-exporter.yaml
~~~

# Deploying Grafana: 
This will create our Grafana deployment and Service which will be exposed using our ingress object. 

We should add thanos-querier as the datasource for our Grafana deployment. Please return to this step once the ingress controller is working and Grafana is accessible - 

##### Click on Add DataSource
1. Set Name: DS_PROMETHEUS 
2. Set Type: Prometheus 
3. Set URL: http://thanos-querier:9090
Save and Test. 

You can now build your custom dashboards or simply import dashboards from grafana.net. Dashboard #315 and #1471 are a very good place to start. 


~~~
kubectl apply -f 11-grafana.yaml
~~~


# Deploying NGINX Ingress: 
~~~
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v0.35.0/deploy/static/provider/aws/deploy.yaml
~~~
source: https://kubernetes.github.io/ingress-nginx/deploy/#aws

Please note that this will install the NGINX ingress behind NLB. it will take about 3 - 4 minutes to register.

Please browse the DNS provided until you see the nginx page on your browser before you continue:
~~~
kubectl describe svc/ingress-nginx-controller -n ingress-nginx
~~~


# Deploying the Ingress Object: 
This is the final piece in the puzzle. This will help expose all our services outside the Kubernetes cluster and help us access them. 

if you are not using real DNS services.
Update you host file with the following:

1. Get the CNAME of you NLB 
~~~
kubectl get service/ingress-nginx-controller -n ingress-nginx |  awk {'print  $4 '} | column -t
~~~
2. ping <CNAME>
The IP the you recived - use as the NLB IP in your host file (LINUX / WINDOWS)

/etc/hosts
~~~
<NLB IP> grafana.monitoring.com thanos-querier.monitoring.com thanos-ruler.monitoring.com alertmanager.monitoring.com prometheus-0.monitoring.com
~~~



